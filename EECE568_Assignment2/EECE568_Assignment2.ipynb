{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EECE568 - Assignment 2"
   ],
   "metadata": {
    "id": "z0ffSZLoVp1F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mount on Google Drive\n"
   ],
   "metadata": {
    "id": "qV_f-VYWoFpy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ],
   "metadata": {
    "id": "HQmqVUUZqbEE",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:30.147488Z",
     "start_time": "2023-10-29T16:14:30.055084Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# %cd gdrive/My Drive/EECE568_Assignment2/"
   ],
   "metadata": {
    "id": "Ha-dMtFxqfT-",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:30.147614Z",
     "start_time": "2023-10-29T16:14:30.058005Z"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "id": "PpLqwKc9UjKl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_7AAgEL2nHGg",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:30.149435Z",
     "start_time": "2023-10-29T16:14:30.061854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on torch.__version__ =  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import standard PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import torchvision module to handle image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(\"Running on torch.__version__ = \", torch.__version__)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# utils\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "id": "TUVmUx8PY4YA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# a function to move tensors from the CPU to the GPU and vice versa\n",
    "def dict_to_device(orig, device):\n",
    "    new = {}\n",
    "    for k,v in orig.items():\n",
    "        new[k] = v.to(device)\n",
    "    return new\n",
    "\n",
    "# a function to make gray-scale images the same shape as color images\n",
    "def gray_to_color(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "# a function to plot a batch of images together\n",
    "def plot_images(img, ax):\n",
    "    img = torchvision.utils.make_grid(img)\n",
    "    npimg = img.numpy()\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))"
   ],
   "metadata": {
    "id": "de-TYlriY6ZE",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:30.149472Z",
     "start_time": "2023-10-29T16:14:30.068682Z"
    }
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1"
   ],
   "metadata": {
    "id": "riEkmtv8VtaX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset and Dataloader"
   ],
   "metadata": {
    "id": "ia3AtMLRU0Zz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UGwIBAY3SYRy",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:31.732496Z",
     "start_time": "2023-10-29T16:14:30.074346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MyDataset/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MyDataset/raw/train-images-idx3-ubyte.gz to ./MyDataset/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MyDataset/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MyDataset/raw/train-labels-idx1-ubyte.gz to ./MyDataset/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MyDataset/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MyDataset/raw/t10k-images-idx3-ubyte.gz to ./MyDataset/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MyDataset/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./MyDataset/raw/t10k-labels-idx1-ubyte.gz to ./MyDataset/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use standard MNIST dataset\n",
    "class MyDataset(torchvision.datasets.MNIST):\n",
    "    def __init__(self, *args, debug=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.debug = debug\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = super().__getitem__(idx)\n",
    "        img = data[0]\n",
    "        label = data[1]\n",
    "        return {'image': img, 'label': label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "dataset = MyDataset(\n",
    "    root = './',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        gray_to_color,\n",
    "    ]),\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=128, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: train, validation, test split\n",
    "\n",
    "The standard MNIST dataset does not provide a validation set. Use 20% of the training data as the validation set.\n",
    "\n",
    "The standard MNIST dataset has a test set, and you can download it similar to downloading the train set, only by setting the train label to 'False'. Use the test set only for final the evaluation."
   ],
   "metadata": {
    "id": "P0lazZkraP8M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: load the test set\n"
   ],
   "metadata": {
    "id": "uJ5ZEChSWYFV",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:31.753767Z",
     "start_time": "2023-10-29T16:14:31.731921Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: split the training data into train and validation, and define the dataloaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None"
   ],
   "metadata": {
    "id": "noQlWs-Fbecq",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:31.760142Z",
     "start_time": "2023-10-29T16:14:31.733971Z"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: print the size of train, validation, and test sets\n"
   ],
   "metadata": {
    "id": "TFgYOumlRZ8W",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:31.760356Z",
     "start_time": "2023-10-29T16:14:31.737045Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network"
   ],
   "metadata": {
    "id": "m-qcpHIZZsOY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3rV9ktECSYRz",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:31.824543Z",
     "start_time": "2023-10-29T16:14:31.741839Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 37\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m: pred_label}\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# You can choose either of the defined networks, or define your own neural net\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m network \u001B[38;5;241m=\u001B[39m \u001B[43mMyNetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(network)\n",
      "File \u001B[0;32m~/Documents/Documents - Jason's MacBook/UBC Courses/EECE568/EECE568_Assignment2/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1156\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1157\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Documents - Jason's MacBook/UBC Courses/EECE568/EECE568_Assignment2/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Documents - Jason's MacBook/UBC Courses/EECE568/EECE568_Assignment2/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/Documents/Documents - Jason's MacBook/UBC Courses/EECE568/EECE568_Assignment2/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1157\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Documents - Jason's MacBook/UBC Courses/EECE568/EECE568_Assignment2/myenv/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# an example of using available models in PyTorch\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet18 = torchvision.models.resnet18(num_classes=10)\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        pred_label = self.resnet18(input_dict['image'])\n",
    "        return {'label': pred_label}\n",
    "\n",
    "# a simple CNN model, implemented from the scratch\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.fc = nn.Linear(32*26*26, 10)\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        x = self.conv1(input_dict['image'])\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        pred_label = nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "        return {'label': pred_label}\n",
    "\n",
    "# You can choose either of the defined networks, or define your own neural net\n",
    "network = MyNetwork().to('cuda')\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "id": "X82PEwzSZvXg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# training loop\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# prepare plotting\n",
    "fig = plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes = fig.subplots(1,3)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    network.train()\n",
    "    for i in range(len(train_loader)):\n",
    "        batch_cpu = next(train_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "\n",
    "        # calculate the loss and backward the gradient\n",
    "        loss = nn.CrossEntropyLoss()(pred['label'], batch_gpu['label'])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if i%100==0:\n",
    "            axes[0].cla()\n",
    "            axes[1].cla()\n",
    "\n",
    "            # plot some sample image inputs\n",
    "            plot_images(batch_cpu['image'][0:1], ax=axes[0])\n",
    "            axes[0].legend()\n",
    "            axes[0].set_title('sample input')\n",
    "\n",
    "            # plot the training error on a log plot\n",
    "            axes[1].plot(train_losses, label='loss')\n",
    "            axes[1].set_yscale('log')\n",
    "            axes[1].set_title('Training loss')\n",
    "            axes[1].set_xlabel('number of gradient iterations')\n",
    "            axes[1].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Training epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), train_losses[-1]))\n",
    "\n",
    "    val_iter = iter(val_loader)\n",
    "    network.eval()\n",
    "    for i in range(len(val_loader)):\n",
    "        batch_cpu = next(val_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "\n",
    "        # calculate the loss\n",
    "        with torch.no_grad():\n",
    "            loss = nn.CrossEntropyLoss()(pred['label'], batch_gpu['label'])\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "        if i%10==0:\n",
    "            axes[2].cla()\n",
    "\n",
    "            # plot the validation error on a log plot\n",
    "            axes[2].plot(val_losses, label='loss')\n",
    "            axes[2].set_yscale('log')\n",
    "            axes[2].set_title('Validation loss')\n",
    "            axes[2].set_xlabel('number of gradient iterations')\n",
    "            axes[2].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Validation epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(val_loader), 100*i//len(val_loader), val_losses[-1]))\n",
    "plt.close('all')"
   ],
   "metadata": {
    "id": "7K_GSHDCVUXB",
    "ExecuteTime": {
     "end_time": "2023-10-29T16:14:31.825680Z",
     "start_time": "2023-10-29T16:14:31.825036Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Evaluation\n",
    "\n",
    "Here we want to report the accuracy of the network's predictions. We have defined a function named 'get_accuracy' that returns the accuracy of the network on its input data.\n",
    "\n",
    "Here, we assumed that the network returns class probabilities as output. If your network returns class indices, you need to change this evaluation function as well.\n",
    "\n",
    "Print the accuracy of your network for the train, validation, and test sets."
   ],
   "metadata": {
    "id": "bLWTsDFJWgbP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_accuracy(network, data_loader):\n",
    "    network.eval()\n",
    "    iterator = iter(data_loader)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(iterator)):\n",
    "        batch_cpu = next(iterator)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)['label'].argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(batch_gpu['label'].view_as(pred)).sum().item()\n",
    "        total += pred.shape[0]\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "id": "XfxpL80cWgPN",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.826778Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: report accuracy on train, validation, and test sets\n"
   ],
   "metadata": {
    "id": "QpLmlfSMVTQT",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.827902Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: Plot the input images and output of your network for a few samples in the test set\n"
   ],
   "metadata": {
    "id": "PorIeQrxYBxo",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.829093Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2"
   ],
   "metadata": {
    "id": "OWduSu89XD1_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset and Dataloader"
   ],
   "metadata": {
    "id": "mOzcfTl5Y_K3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBL6xWU0Y_K7",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.830261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use standard MNIST dataset\n",
    "class AnomalyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.digits = {0:[], 1:[], 2:[], 3:[], 4:[]}\n",
    "        for idx in range(len(self.data)):\n",
    "            digit = self.data[idx][1]\n",
    "            if digit in self.digits.keys():\n",
    "                self.digits[digit].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        norm, anomaly = random.sample(self.digits.keys(), k=2)\n",
    "        anomaly_loc, = random.sample([0, 1, 2], k=1)\n",
    "        norm1_idx, norm2_idx = random.sample(self.digits[norm], k=2)\n",
    "        anomaly_idx, = random.sample(self.digits[anomaly], k=1)\n",
    "        if anomaly_loc == 0:\n",
    "            img0 = self.data[anomaly_idx][0]\n",
    "            img1 = self.data[norm1_idx][0]\n",
    "            img2 = self.data[norm2_idx][0]\n",
    "        elif anomaly_loc == 1:\n",
    "            img0 = self.data[norm1_idx][0]\n",
    "            img1 = self.data[anomaly_idx][0]\n",
    "            img2 = self.data[norm2_idx][0]\n",
    "        elif anomaly_loc == 2:\n",
    "            img0 = self.data[norm1_idx][0]\n",
    "            img1 = self.data[norm2_idx][0]\n",
    "            img2 = self.data[anomaly_idx][0]\n",
    "\n",
    "        return {'img0': img0, 'img1': img1, 'img2': img2, 'index': anomaly_loc}\n",
    "\n",
    "dataset = AnomalyDataset(torchvision.datasets.MNIST(\n",
    "    root = './',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        gray_to_color,\n",
    "    ])\n",
    "))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=128, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print('Selected digits and their frequencies:')\n",
    "for key, value in loader.dataset.digits.items():\n",
    "    print(key, len(value))"
   ],
   "metadata": {
    "id": "ozpjw8-ccVje",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.831483Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: train, validation, test split\n",
    "\n",
    "Use 20% of the training data as the validation set.\n",
    "\n",
    "The standard MNIST dataset has a test set, and you can download it similar to downloading the train set, only by setting the train label to 'False'. Use the test set only for final the evaluation."
   ],
   "metadata": {
    "id": "spH6m-qn1TSZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: load the test set\n"
   ],
   "metadata": {
    "id": "7ffVjsnYcBYh",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.832710Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: define the dataloaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None"
   ],
   "metadata": {
    "id": "9uZIh9v61TSZ",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.833894Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: print the size of train, validation, and test sets\n"
   ],
   "metadata": {
    "id": "0dx5H2Z7SY1H",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.835051Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Network\n",
    "\n",
    "Implement a neural network that takes three images as input and returns the index of the image with the different digit.\n",
    "\n",
    "Your network should take a python dictionary as input and extract the input images from it. Your network should return a python dictionaly containing a key named 'index'."
   ],
   "metadata": {
    "id": "7wYMpiDnsfLI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey8RaNhWsfLO",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.836247Z"
    }
   },
   "outputs": [],
   "source": [
    "# ToDo: Implement your neural network from scratch\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        # ToDo: code here\n",
    "\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        img0 = input_dict['img0']\n",
    "        img1 = input_dict['img1']\n",
    "        img2 = input_dict['img2']\n",
    "\n",
    "        # ToDo: code here\n",
    "\n",
    "        # Hint: use padding in conv layers to adjust the dimensions\n",
    "        # Hint: max-pooling and ReLU layers can be useful\n",
    "        # Hint: choose a suitable activation function for the last layer\n",
    "\n",
    "\n",
    "        return {'index': pred_index}\n",
    "\n",
    "network = MyNetwork().to('cuda')\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Training"
   ],
   "metadata": {
    "id": "tXtc6rhhtwGC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: define a suitable loss function\n",
    "def my_loss_function(predicted_index, target_index):\n",
    "\n",
    "  # code here\n",
    "\n",
    "  return loss"
   ],
   "metadata": {
    "id": "li1mo8YCUeif",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.837432Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# training loop\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# prepare plotting\n",
    "fig = plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes = fig.subplots(1,3)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    network.train()\n",
    "    for i in range(len(train_loader)):\n",
    "        batch_cpu = next(train_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "\n",
    "        # calculate the loss and backward the gradient\n",
    "        loss = my_loss_function(pred['index'], batch_gpu['index'])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if i%100==0:\n",
    "            axes[0].cla()\n",
    "            axes[1].cla()\n",
    "\n",
    "            # plot some sample image inputs\n",
    "            plot_images(torch.cat((batch_cpu['img0'][0:1], batch_cpu['img1'][0:1], batch_cpu['img2'][0:1]), 0), ax=axes[0])\n",
    "            axes[0].legend()\n",
    "            axes[0].set_title('sample input')\n",
    "\n",
    "            # plot the training error on a log plot\n",
    "            axes[1].plot(train_losses, label='loss')\n",
    "            axes[1].set_yscale('log')\n",
    "            axes[1].set_title('Training loss')\n",
    "            axes[1].set_xlabel('number of gradient iterations')\n",
    "            axes[1].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Training epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), train_losses[-1]))\n",
    "\n",
    "    val_iter = iter(val_loader)\n",
    "    network.eval()\n",
    "    for i in range(len(val_loader)):\n",
    "        batch_cpu = next(val_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "\n",
    "        # calculate the loss\n",
    "        with torch.no_grad():\n",
    "            loss = my_loss_function(pred['index'], batch_gpu['index'])\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "        if i%10==0:\n",
    "            axes[2].cla()\n",
    "\n",
    "            # plot the validation error on a log plot\n",
    "            axes[2].plot(val_losses, label='loss')\n",
    "            axes[2].set_yscale('log')\n",
    "            axes[2].set_title('Validation loss')\n",
    "            axes[2].set_xlabel('number of gradient iterations')\n",
    "            axes[2].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Validation epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(val_loader), 100*i//len(val_loader), val_losses[-1]))\n",
    "plt.close('all')"
   ],
   "metadata": {
    "id": "Oz7ycBagtwGD",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.838789Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Evaluation\n",
    "\n",
    "Here we want to report the accuracy of the network's predictions. We have defined a function named 'get_accuracy' that returns the accuracy of the network on its input data.\n",
    "\n",
    "Here, we assumed that the network returns class probabilities as output. If your network returns class indices, you need to change this evaluation function as well.\n",
    "\n",
    "Print the accuracy of your network for the train, validation, and test sets."
   ],
   "metadata": {
    "id": "FZgyhGeBym3C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_accuracy(network, data_loader):\n",
    "    network.eval()\n",
    "    iterator = iter(data_loader)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(iterator)):\n",
    "        batch_cpu = next(iterator)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)['index'].argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(batch_gpu['index'].view_as(pred)).sum().item()\n",
    "        total += pred.shape[0]\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "id": "hK6tKCwKym3M",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.839732Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: report accuracy on train, validation, and test sets\n"
   ],
   "metadata": {
    "id": "y3awYYf41IJ8",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.840485Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: Plot the input images and output of your network for a few samples in the test set\n"
   ],
   "metadata": {
    "id": "0TZ_ZGTTzEfe",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.841082Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3"
   ],
   "metadata": {
    "id": "GwAZBm99hfSk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and Tokenize Data"
   ],
   "metadata": {
    "id": "M516Ld3Lfnm9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('./imdb_processed.csv')\n",
    "data.head()"
   ],
   "metadata": {
    "id": "TK_xBX9jbi_M",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.841650Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# obtain list of words\n",
    "words = ' '.join(data.processed.values).split()\n",
    "\n",
    "# check our list\n",
    "words[30:40]"
   ],
   "metadata": {
    "id": "YQ5YSgrjbjDj",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.842324Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# build vocabulary\n",
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "token2word = dict(enumerate(vocab, 1))\n",
    "token2word[0] = '<PAD>'\n",
    "word2token = {word: id for id, word in token2word.items()}"
   ],
   "metadata": {
    "id": "pvHO-JBcf_ZJ",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.843007Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenize reviews\n",
    "reviews = data.processed.values\n",
    "reviews_tokenized = [[word2token[word] for word in review.split()] for review in tqdm(reviews)]\n",
    "\n",
    "# padding sequences\n",
    "def pad_features(reviews, pad_id, seq_length):\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "    for i, row in enumerate(reviews):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "features = pad_features(reviews_tokenized, pad_id=word2token['<PAD>'], seq_length=256)\n",
    "\n",
    "print('number of reviews:', len(reviews_tokenized))\n",
    "print('seq_length:', len(features[0]))\n",
    "\n",
    "# print first-5 words of first 3 reviews\n",
    "print('\\n first-five words of the first-three reviews:')\n",
    "print('===============')\n",
    "features[:3, :5]"
   ],
   "metadata": {
    "id": "KmutxG8If_be",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.843768Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataloader"
   ],
   "metadata": {
    "id": "10gym2O8k4Jr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# we use 70%, 15%, 15% for train, validation, and test sets\n",
    "train_size = .7\n",
    "val_size = .5\n",
    "labels = data.label.to_numpy()\n",
    "\n",
    "# make train set\n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, remain_x = features[:split_id], features[split_id:]\n",
    "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "# make val and test set\n",
    "split_val_id = int(len(remain_x) * val_size)\n",
    "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
    "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n",
    "\n",
    "# print out the shape\n",
    "print('Train set: {}'.format(train_x.shape))\n",
    "print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))"
   ],
   "metadata": {
    "id": "fUtpw4oSjwOq",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.844402Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.data = torch.utils.data.TensorDataset(torch.from_numpy(reviews), torch.from_numpy(labels).float())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.data[index][0]\n",
    "        label = self.data[index][1]\n",
    "\n",
    "        return {'review': review, 'label': label}\n",
    "\n",
    "\n",
    "train_set = MyDataset(train_x, train_y)\n",
    "val_set = MyDataset(val_x, val_y)\n",
    "test_set = MyDataset(test_x, test_y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=128)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=128)"
   ],
   "metadata": {
    "id": "XPqwDm8dsqle",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.845087Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Network\n",
    "\n",
    "Implement a neural network based on Long short-term memory (LSTM) that takes a series of words (a review) as input and returns the label of the review indicating whether it is positive (1) or negative (0).\n",
    "\n",
    "Your network should take a python dictionary as input and extract the input sentences from it. Your network should return a python dictionaly containing a key named 'label'."
   ],
   "metadata": {
    "id": "fKcwr-Z7lwyM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: Implement your neural network\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        # ToDo: code here\n",
    "\n",
    "        # Hint: start with an embedding layer\n",
    "        # Hint: using dropout might be useful\n",
    "        # Hint: choose a suitable activation function for the last layer\n",
    "\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        x = input_dict['review']\n",
    "\n",
    "        # ToDo: code here\n",
    "\n",
    "        return {'label': pred_label}\n",
    "\n",
    "network = SentimentLSTM().to('cuda')\n",
    "print(network)"
   ],
   "metadata": {
    "id": "J4SS_pxBjwTt",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.845847Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Training"
   ],
   "metadata": {
    "id": "uAl8aDrZq5yo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: define a suitable loss function\n",
    "def my_loss_function(predicted_label, target_label):\n",
    "\n",
    "  # code here\n",
    "\n",
    "  return loss"
   ],
   "metadata": {
    "id": "aywwSvIWaNXl",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.846512Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# training loop\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# prepare plotting\n",
    "fig = plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes = fig.subplots(1,2)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    network.train()\n",
    "    for i in range(len(train_loader)):\n",
    "        batch_cpu = next(train_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "\n",
    "        # calculate the loss and backward the gradient\n",
    "        loss = my_loss_function(pred['label'], batch_gpu['label'])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if i%100==0:\n",
    "            axes[0].cla()\n",
    "\n",
    "            # plot the training error on a log plot\n",
    "            axes[0].plot(train_losses, label='loss')\n",
    "            axes[0].set_yscale('log')\n",
    "            axes[0].set_title('Training loss')\n",
    "            axes[0].set_xlabel('number of gradient iterations')\n",
    "            axes[0].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Training epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), train_losses[-1]))\n",
    "\n",
    "    val_iter = iter(val_loader)\n",
    "    network.eval()\n",
    "    for i in range(len(val_loader)):\n",
    "        batch_cpu = next(val_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "\n",
    "        # calculate the loss\n",
    "        with torch.no_grad():\n",
    "            loss = my_loss_function(pred['label'], batch_gpu['label'])\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "        if i%10==0:\n",
    "            axes[1].cla()\n",
    "\n",
    "            # plot the validation error on a log plot\n",
    "            axes[1].plot(val_losses, label='loss')\n",
    "            axes[1].set_yscale('log')\n",
    "            axes[1].set_title('Validation loss')\n",
    "            axes[1].set_xlabel('number of gradient iterations')\n",
    "            axes[1].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Validation epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(val_loader), 100*i//len(val_loader), val_losses[-1]))\n",
    "plt.close('all')"
   ],
   "metadata": {
    "id": "6Y2ytRQVq6KJ",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.847189Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ToDo: Evaluation\n",
    "\n",
    "Here we want to report the accuracy of the network's predictions. We have defined a function named 'get_accuracy' that returns the accuracy of the network on its input data.\n",
    "\n",
    "Here, we assumed that the network returns class probabilities as output. If your network returns class indices, you might need to change this evaluation function as well.\n",
    "\n",
    "Print the accuracy of your network for the train, validation, and test sets."
   ],
   "metadata": {
    "id": "PGSVFB0g4UJo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_accuracy(network, data_loader):\n",
    "    network.eval()\n",
    "    iterator = iter(data_loader)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(iterator)):\n",
    "        batch_cpu = next(iterator)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = network(batch_gpu)['label']\n",
    "        binary_pred = torch.where(pred < 0.5, 0, 1).squeeze(-1)\n",
    "        correct += binary_pred.eq(batch_gpu['label']).sum().item()\n",
    "        total += binary_pred.shape[0]\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "id": "oyRNmKET4UJt",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.847742Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: report accuracy on train, validation, and test sets\n"
   ],
   "metadata": {
    "id": "MmHOEPBc4UJt",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.848397Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ToDo: print the input review and output of your network for one positive\n",
    "#       example and one negative example in the test set\n"
   ],
   "metadata": {
    "id": "hXbG8MEo4UJt",
    "ExecuteTime": {
     "start_time": "2023-10-29T16:14:31.848999Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
